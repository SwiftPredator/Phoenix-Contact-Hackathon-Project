{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_profiling\n",
    "from visualize import visualize_rooms\n",
    "from preprocessing import preprocess_df, merge_data\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from model import train_model\n",
    "from autots import AutoTS\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autots.evaluator.benchmark import Benchmark\n",
    "bench = Benchmark()\n",
    "bench.run(n_jobs=\"auto\", times=3)\n",
    "bench.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and preprocess\n",
    "df = pd.read_csv(\"data/train/data.csv\", sep=\";\")\n",
    "# merge dataset with weather data\n",
    "df = merge_data(df)\n",
    "# discretize, impute etc.\n",
    "df = preprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "dfp.to_csv(\"./data/train/preprocessed_train.csv\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.12824612\n",
      "Iteration 2, loss = 0.29221966\n",
      "Iteration 3, loss = 0.22378249\n",
      "Iteration 4, loss = 0.19912970\n",
      "Iteration 5, loss = 0.18698228\n",
      "Iteration 6, loss = 0.17930021\n",
      "Iteration 7, loss = 0.17403015\n",
      "Iteration 8, loss = 0.17046356\n",
      "Iteration 9, loss = 0.16748388\n",
      "Iteration 10, loss = 0.16504452\n",
      "Iteration 11, loss = 0.16319854\n",
      "Iteration 12, loss = 0.16072004\n",
      "Iteration 13, loss = 0.15939499\n",
      "Iteration 14, loss = 0.15775809\n",
      "Iteration 15, loss = 0.15660681\n",
      "Iteration 16, loss = 0.15510819\n",
      "Iteration 17, loss = 0.15401781\n",
      "Iteration 18, loss = 0.15316896\n",
      "Iteration 19, loss = 0.15219019\n",
      "Iteration 20, loss = 0.15147132\n",
      "Iteration 21, loss = 0.15073778\n",
      "Iteration 22, loss = 0.14995406\n",
      "Iteration 23, loss = 0.14918801\n",
      "Iteration 24, loss = 0.14902369\n",
      "Iteration 25, loss = 0.14813625\n",
      "Iteration 26, loss = 0.14774502\n",
      "Iteration 27, loss = 0.14762732\n",
      "Iteration 28, loss = 0.14694145\n",
      "Iteration 29, loss = 0.14662518\n",
      "Iteration 30, loss = 0.14629679\n",
      "Iteration 31, loss = 0.14596139\n",
      "Iteration 32, loss = 0.14551114\n",
      "Iteration 33, loss = 0.14505207\n",
      "Iteration 34, loss = 0.14462658\n",
      "Iteration 35, loss = 0.14412622\n",
      "Iteration 36, loss = 0.14412514\n",
      "Iteration 37, loss = 0.14327744\n",
      "Iteration 38, loss = 0.14329096\n",
      "Iteration 39, loss = 0.14286338\n",
      "Iteration 40, loss = 0.14281780\n",
      "Iteration 41, loss = 0.14244252\n",
      "Iteration 42, loss = 0.14226267\n",
      "Iteration 43, loss = 0.14190885\n",
      "Iteration 44, loss = 0.14158535\n",
      "Iteration 45, loss = 0.14132296\n",
      "Iteration 46, loss = 0.14104964\n",
      "Iteration 47, loss = 0.14077538\n",
      "Iteration 48, loss = 0.14057699\n",
      "Iteration 49, loss = 0.14040517\n",
      "Iteration 50, loss = 0.14006745\n",
      "Iteration 51, loss = 0.14002283\n",
      "Iteration 52, loss = 0.13982525\n",
      "Iteration 53, loss = 0.13946966\n",
      "Iteration 54, loss = 0.13916454\n",
      "Iteration 55, loss = 0.13888323\n",
      "Iteration 56, loss = 0.13846886\n",
      "Iteration 57, loss = 0.13845855\n",
      "Iteration 58, loss = 0.13830348\n",
      "Iteration 59, loss = 0.13792949\n",
      "Iteration 60, loss = 0.13745938\n",
      "Iteration 61, loss = 0.13732344\n",
      "Iteration 62, loss = 0.13734711\n",
      "Iteration 63, loss = 0.13683374\n",
      "Iteration 64, loss = 0.13692648\n",
      "Iteration 65, loss = 0.13663028\n",
      "Iteration 66, loss = 0.13653274\n",
      "Iteration 67, loss = 0.13627674\n",
      "Iteration 68, loss = 0.13616781\n",
      "Iteration 69, loss = 0.13640610\n",
      "Iteration 70, loss = 0.13606430\n",
      "Iteration 71, loss = 0.13587127\n",
      "Iteration 72, loss = 0.13577724\n",
      "Iteration 73, loss = 0.13574867\n",
      "Iteration 74, loss = 0.13547662\n",
      "Iteration 75, loss = 0.13534876\n",
      "Iteration 76, loss = 0.13504136\n",
      "Iteration 77, loss = 0.13512400\n",
      "Iteration 78, loss = 0.13492428\n",
      "Iteration 79, loss = 0.13480345\n",
      "Iteration 80, loss = 0.13481104\n",
      "Iteration 81, loss = 0.13429594\n",
      "Iteration 82, loss = 0.13466325\n",
      "Iteration 83, loss = 0.13439917\n",
      "Iteration 84, loss = 0.13419708\n",
      "Iteration 85, loss = 0.13414520\n",
      "Iteration 86, loss = 0.13434202\n",
      "Iteration 87, loss = 0.13368090\n",
      "Iteration 88, loss = 0.13402829\n",
      "Iteration 89, loss = 0.13378527\n",
      "Iteration 90, loss = 0.13396763\n",
      "Iteration 91, loss = 0.13341164\n",
      "Iteration 92, loss = 0.13324550\n",
      "Iteration 93, loss = 0.13326926\n",
      "Iteration 94, loss = 0.13333582\n",
      "Iteration 95, loss = 0.13302494\n",
      "Iteration 96, loss = 0.13293188\n",
      "Iteration 97, loss = 0.13307401\n",
      "Iteration 98, loss = 0.13290231\n",
      "Iteration 99, loss = 0.13272151\n",
      "Iteration 100, loss = 0.13287867\n",
      "Iteration 101, loss = 0.13264566\n",
      "Iteration 102, loss = 0.13257250\n",
      "Iteration 103, loss = 0.13230611\n",
      "Iteration 104, loss = 0.13220230\n",
      "Iteration 105, loss = 0.13266766\n",
      "Iteration 106, loss = 0.13220810\n",
      "Iteration 107, loss = 0.13245605\n",
      "Iteration 108, loss = 0.13228380\n",
      "Iteration 109, loss = 0.13234808\n",
      "Iteration 110, loss = 0.13156041\n",
      "Iteration 111, loss = 0.13214117\n",
      "Iteration 112, loss = 0.13130758\n",
      "Iteration 113, loss = 0.13193489\n",
      "Iteration 114, loss = 0.13148438\n",
      "Iteration 115, loss = 0.13176713\n",
      "Iteration 116, loss = 0.13160334\n",
      "Iteration 117, loss = 0.13134961\n",
      "Iteration 118, loss = 0.13162341\n",
      "Iteration 119, loss = 0.13141097\n",
      "Iteration 120, loss = 0.13105583\n",
      "Iteration 121, loss = 0.13146776\n",
      "Iteration 122, loss = 0.13105190\n",
      "Iteration 123, loss = 0.13125358\n",
      "Iteration 124, loss = 0.13093520\n",
      "Iteration 125, loss = 0.13092907\n",
      "Iteration 126, loss = 0.13089363\n",
      "Iteration 127, loss = 0.13112952\n",
      "Iteration 128, loss = 0.13098880\n",
      "Iteration 129, loss = 0.13067798\n",
      "Iteration 130, loss = 0.13089026\n",
      "Iteration 131, loss = 0.13067292\n",
      "Iteration 132, loss = 0.13099822\n",
      "Iteration 133, loss = 0.13079052\n",
      "Iteration 134, loss = 0.13034692\n",
      "Iteration 135, loss = 0.13089147\n",
      "Iteration 136, loss = 0.13076720\n",
      "Iteration 137, loss = 0.13079033\n",
      "Iteration 138, loss = 0.13062064\n",
      "Iteration 139, loss = 0.13096772\n",
      "Iteration 140, loss = 0.13032898\n",
      "Iteration 141, loss = 0.13072261\n",
      "Iteration 142, loss = 0.13054258\n",
      "Iteration 143, loss = 0.13032159\n",
      "Iteration 144, loss = 0.13012376\n",
      "Iteration 145, loss = 0.13032402\n",
      "Iteration 146, loss = 0.13051534\n",
      "Iteration 147, loss = 0.13032730\n",
      "Iteration 148, loss = 0.13030290\n",
      "Iteration 149, loss = 0.13016356\n",
      "Iteration 150, loss = 0.13006716\n",
      "Iteration 151, loss = 0.13023706\n",
      "Iteration 152, loss = 0.13022818\n",
      "Iteration 153, loss = 0.12984949\n",
      "Iteration 154, loss = 0.13029278\n",
      "Iteration 155, loss = 0.13004991\n",
      "Iteration 156, loss = 0.12994841\n",
      "Iteration 157, loss = 0.13009788\n",
      "Iteration 158, loss = 0.12993733\n",
      "Iteration 159, loss = 0.13008294\n",
      "Iteration 160, loss = 0.12952163\n",
      "Iteration 161, loss = 0.12974392\n",
      "Iteration 162, loss = 0.12957501\n",
      "Iteration 163, loss = 0.12987019\n",
      "Iteration 164, loss = 0.12962584\n",
      "Iteration 165, loss = 0.12971258\n",
      "Iteration 166, loss = 0.13000174\n",
      "Iteration 167, loss = 0.12952641\n",
      "Iteration 168, loss = 0.12986596\n",
      "Iteration 169, loss = 0.12947175\n",
      "Iteration 170, loss = 0.12968481\n",
      "Iteration 171, loss = 0.12939663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "df = pd.read_csv(\"./data/train/preprocessed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Unnamed: 0': 'timestamp'})\n",
    "#df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss, RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby(\"Room\")\n",
    "split = [g.get_group(x) for x in g.groups]\n",
    "dfone = split[0].reset_index(drop=True)\n",
    "dfone[\"time_idx\"] = dfone.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfone.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfone[\"RoomTemperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 4*24*7\n",
    "max_encoder_length = 5*4*24*7\n",
    "training_cutoff = dfone[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    dfone[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"RoomTemperature\",\n",
    "    group_ids=[\"Room\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],\n",
    "    #static_reals=[\"Room\"],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_unknown_reals=[\"RoomTemperature\"],\n",
    "    #time_varying_known_reals=[\n",
    "    #\"time_idx\",  'AirqualityPerc', 'HeaterPerc',\n",
    "    #    'CoolerPerc', 'TempSupplyAir', 'RelativeHumiditySupplyAir',\n",
    "    #    'HeatingPower', 'CoolingPower', 'AirTemperature', 'WindDirection',\n",
    "    #    'BrightnessNorth', 'BrightnessEast', 'BrightnessSouth',\n",
    "    #    'BrightnessWest', 'dwpt', 'rhum',\n",
    "    #    'prcp', 'wspd', 'tsun'],\n",
    "    # time_varying_unknown_categoricals=[],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, dfone, predict=True, stop_randomization=True)\n",
    "batch_size = 32  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=10)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 21.5k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    hidden_size=16,\n",
    "    lstm_layers=2,\n",
    "    learning_rate=0.06760829753919811,\n",
    "    dropout=0.1,\n",
    "    loss=RMSE(),\n",
    "    attention_head_size=4\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.532465699366784\n"
     ]
    }
   ],
   "source": [
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    gpus=0,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    #fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0     \n",
      "3  | prescalers                         | ModuleDict                      | 80    \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.7 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.2 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 528   \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 4.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 4.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "21.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.5 K    Total params\n",
      "0.086     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PossibleUserWarning: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd1a9428a6b49a6be1e35848147e647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model then:\n",
    "# set n=1 if you only want your best model\n",
    "model.export_template(\"my_models.csv\", models='best', n=15, max_per_model_class=3)\n",
    "\n",
    "# later on a new session\n",
    "# you can set `max_generations=0` in model, and then it will only attempt the imported models\n",
    "# model = model.import_template(\"my_models.csv\", method='only')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a2bf5c915fa092a6c93bd18fc1b38f3d48b36f64db4e1baffc8c1df036645e0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
