{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_profiling\n",
    "from visualize import visualize_rooms\n",
    "from preprocessing import preprocess_df, merge_data\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from model import train_model\n",
    "from autots import AutoTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning AverageValueNaive\n",
      "Beginning SectionalMotif\n",
      "Beginning NVAR\n",
      "Beginning Datepart RandomForest\n",
      "Beginning Datepart SVM\n",
      "Beginning Theta\n",
      "Beginning ARIMA\n",
      "Beginning Multivariate KNN\n",
      "Beginning MLP\n",
      "Beginning KerasRNN\n",
      "tensorflow failed with: ModuleNotFoundError(\"No module named 'keras'\")\n",
      "Beginning KerasCNN\n",
      "tensorflow CNN failed with: ModuleNotFoundError(\"No module named 'keras'\")\n",
      "Beginning GluonTS\n",
      "gluonts failed with: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.')\n",
      "Beginning Prophet\n",
      "prophet failed with: ModuleNotFoundError(\"No module named 'prophet'\")\n",
      "Beginning AverageValueNaive\n",
      "Beginning SectionalMotif\n",
      "Beginning NVAR\n",
      "Beginning Datepart RandomForest\n",
      "Beginning Datepart SVM\n",
      "Beginning Theta\n",
      "Beginning ARIMA\n",
      "Beginning Multivariate KNN\n",
      "Beginning MLP\n",
      "Beginning KerasRNN\n",
      "tensorflow failed with: ModuleNotFoundError(\"No module named 'keras'\")\n",
      "Beginning KerasCNN\n",
      "tensorflow CNN failed with: ModuleNotFoundError(\"No module named 'keras'\")\n",
      "Beginning GluonTS\n",
      "gluonts failed with: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.')\n",
      "Beginning Prophet\n",
      "prophet failed with: ModuleNotFoundError(\"No module named 'prophet'\")\n",
      "Beginning AverageValueNaive\n",
      "Beginning SectionalMotif\n",
      "Beginning NVAR\n",
      "Beginning Datepart RandomForest\n",
      "Beginning Datepart SVM\n",
      "Beginning Theta\n",
      "Beginning ARIMA\n",
      "Beginning Multivariate KNN\n",
      "Beginning MLP\n",
      "Beginning KerasRNN\n",
      "tensorflow failed with: ModuleNotFoundError(\"No module named 'keras'\")\n",
      "Beginning KerasCNN\n",
      "tensorflow CNN failed with: ModuleNotFoundError(\"No module named 'keras'\")\n",
      "Beginning GluonTS\n",
      "gluonts failed with: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.')\n",
      "Beginning Prophet\n",
      "prophet failed with: ModuleNotFoundError(\"No module named 'prophet'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'version': '0.4.0',\n",
       " 'platform': 'Windows-10-10.0.19044-SP0',\n",
       " 'node': 'DESKTOP-2GUVNOI',\n",
       " 'python_version': '3.9.12',\n",
       " 'n_jobs': 'auto',\n",
       " 'times': 3,\n",
       " 'avg_naive_runtime': 0.9359801333333356,\n",
       " 'sect_motif_runtime': 5.272833799999991,\n",
       " 'nvar_runtime': 6.925552466666669,\n",
       " 'datepart_trees_runtime': 4.900474333333335,\n",
       " 'datepart_svm_runtime': 6.105861800000004,\n",
       " 'multivariate_knn_runtime': 9.749766833333334,\n",
       " 'theta_runtime': 9.912768633333334,\n",
       " 'arima_runtime': 9.839136566666667,\n",
       " 'sklearn_mlp_runtime': 1.449435100000007,\n",
       " 'total_runtime': 55.09180966666668,\n",
       " 'tensorflow_rnn_runtime': 0.0,\n",
       " 'tensorflow_cnn_runtime': 0.0,\n",
       " 'gluonts_runtime': 0.0,\n",
       " 'prophet_runtime': 0.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autots.evaluator.benchmark import Benchmark\n",
    "bench = Benchmark()\n",
    "bench.run(n_jobs=\"auto\", times=3)\n",
    "bench.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.device_count()\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DtypeWarning: Columns (11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140253, 6) (848411, 20)\n"
     ]
    }
   ],
   "source": [
    "# load dataset and preprocess\n",
    "df = pd.read_csv(\"data/train/data.csv\", sep=\";\")\n",
    "# merge dataset with weather data\n",
    "df = merge_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Room                         0\n",
      "RoomTemperature              0\n",
      "AirqualityPerc               0\n",
      "HeaterPerc                   0\n",
      "CoolerPerc                   0\n",
      "TempSupplyAir                0\n",
      "RelativeHumiditySupplyAir    0\n",
      "HeatingPower                 0\n",
      "CoolingPower                 0\n",
      "AirTemperature               0\n",
      "WindDirection                0\n",
      "BrightnessNorth              0\n",
      "BrightnessEast               0\n",
      "BrightnessSouth              0\n",
      "BrightnessWest               0\n",
      "Beamerstate                  0\n",
      "BucketAttendees              0\n",
      "dwpt                         0\n",
      "rhum                         0\n",
      "prcp                         0\n",
      "wspd                         0\n",
      "tsun                         0\n",
      "coco                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# discretize, impute etc.\n",
    "dfp = preprocess_df(df.copy())\n",
    "print(dfp.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "import torch\n",
    "dfp.to_csv(\"./data/train/preprocessed_train.csv\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "df = pd.read_csv(\"./data/train/preprocessed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Unnamed: 0': 'timestamp'})\n",
    "#df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss, RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby(\"Room\")\n",
    "split = [g.get_group(x) for x in g.groups]\n",
    "dfone = split[0].reset_index(drop=True)\n",
    "dfone[\"time_idx\"] = dfone.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfone.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         21.299999\n",
      "1         21.299999\n",
      "2         21.299999\n",
      "3         21.299999\n",
      "4         21.299999\n",
      "            ...    \n",
      "142671    18.500000\n",
      "142672    18.500000\n",
      "142673    18.500000\n",
      "142674    18.500000\n",
      "142675    18.500000\n",
      "Name: RoomTemperature, Length: 142676, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(dfone[\"RoomTemperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 4*24*7\n",
    "max_encoder_length = 2*4*24*7\n",
    "training_cutoff = dfone[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    dfone[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"RoomTemperature\",\n",
    "    group_ids=[\"Room\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],\n",
    "    #static_reals=[\"Room\"],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_unknown_reals=[\"RoomTemperature\"],\n",
    "    #time_varying_known_reals=[\n",
    "    #\"time_idx\",  'AirqualityPerc', 'HeaterPerc',\n",
    "    #    'CoolerPerc', 'TempSupplyAir', 'RelativeHumiditySupplyAir',\n",
    "    #    'HeatingPower', 'CoolingPower', 'AirTemperature', 'WindDirection',\n",
    "    #    'BrightnessNorth', 'BrightnessEast', 'BrightnessSouth',\n",
    "    #    'BrightnessWest', 'dwpt', 'rhum',\n",
    "    #    'prcp', 'wspd', 'tsun'],\n",
    "    # time_varying_unknown_categoricals=[],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, dfone, predict=True, stop_randomization=True)\n",
    "batch_size = 64  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=3)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0261896848678589"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 75.3k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    hidden_size=64,\n",
    "    lstm_layers=2,\n",
    "    learning_rate=0.01,\n",
    "    dropout=0.1,\n",
    "    loss=RMSE(),\n",
    "    attention_head_size=4,\n",
    "    reduce_on_plateau_patience=100,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    gpus=1,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    #fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0     \n",
      "3  | prescalers                         | ModuleDict                      | 80    \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.9 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 896   \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 16.9 K\n",
      "12 | lstm_decoder                       | LSTM                            | 16.9 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.6 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K \n",
      "20 | output_layer                       | Linear                          | 33    \n",
      "----------------------------------------------------------------------------------------\n",
      "75.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.3 K    Total params\n",
      "0.301     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca9e904eb5f4b4a9a0ebc7cb924d6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PossibleUserWarning: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f88496edc84a37ab8351387f356937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973b42435ca340dead1832a85a707c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d6f79be885475c95cee3e3201505c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323b0b1de2f64f95bb290794936f535a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc68a608561b4c41b5a7c259d96e519b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08a01373753426c8f93795e86e4138f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c384facf80cb456c8a51d9b8964c6ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202f7a39494b447fb18e36f8c1531355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4324ee5cbc4432ca99d22fcbfb4b770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519816b75aa74c90a939691eaf00ca74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001B0E3B4BF70>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paul\\miniconda3\\envs\\hackathon-phoenix\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\paul\\miniconda3\\envs\\hackathon-phoenix\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = df.copy()\n",
    "df_ts = df_ts.reset_index().rename(columns={'index':'timestamp'})\n",
    "print(df_ts.head(2))\n",
    "# Evaluate different simple time series models\n",
    "model = AutoTS(\n",
    "    forecast_length=3,\n",
    "    frequency='infer',\n",
    "    ensemble='simple',\n",
    "    max_generations=3,\n",
    "    num_validations=2,\n",
    "    n_jobs='auto'\n",
    ")\n",
    "\n",
    "model.fit(df_ts, date_col='timestamp', value_col='RoomTemperature', id_col=\"Room\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a2bf5c915fa092a6c93bd18fc1b38f3d48b36f64db4e1baffc8c1df036645e0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
